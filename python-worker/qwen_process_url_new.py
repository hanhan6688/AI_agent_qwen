import os
import time
import random
import json
import re
import logging
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dashscope import MultiModalConversation
from pathlib import Path
import dashscope
from dotenv import load_dotenv
from typing import Tuple, Optional, Dict, Any

load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('MaterialExtractor')

# ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½prompt.txt
PROMPT_PATH = Path(__file__).parent / "prompt.txt"
if not PROMPT_PATH.exists():
    logger.error(f"prompt.txt æ–‡ä»¶ä¸å­˜åœ¨: {PROMPT_PATH}")
    PROMPT_TXT = ""
else:
    PROMPT_TXT = open(PROMPT_PATH, encoding="utf-8").read()
    if not PROMPT_TXT.strip():
        logger.warning("prompt.txt æ–‡ä»¶ä¸ºç©ºï¼ŒQwen å°†æ— æ³•æ­£ç¡®æå–æ•°æ®ï¼")

# æ¨¡å‹é…ç½®
MODEL_VL = "qwen3-vl-plus"             # è§†è§‰æ¨¡å‹ï¼ˆå¸¦å›¾è¡¨ï¼‰- æ™®é€šç‰ˆ
MODEL_LONG = "qwen-long"              # é•¿æ–‡æœ¬æ¨¡å‹ï¼ˆæ— å›¾è¡¨ï¼‰- æ™®é€šç‰ˆ
MODEL_PRO = "qwen3.5-plus"            # ä¸“ä¸šç‰ˆæ¨¡å‹ - æ›´å¼ºå¤§çš„æ¨ç†èƒ½åŠ›

# qwen3-vl-plus å‚æ•°é…ç½®ï¼ˆæ™®é€šç‰ˆ-è§†è§‰ï¼‰
MAX_CONTEXT_LENGTH_VL = 254000        # qwen3-vl-plus æœ€å¤§è¾“å…¥é•¿åº¦ 254K
MAX_CONTEXT_LENGTH_LONG = 1000000     # qwen-long æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ1M tokensï¼‰

# qwen3.5-plus å‚æ•°é…ç½®ï¼ˆä¸“ä¸šç‰ˆï¼‰
MAX_CONTEXT_LENGTH_PRO = 991000       # qwen3.5-plus æœ€å¤§è¾“å…¥é•¿åº¦ 991K
MAX_RPM_PRO = 30000                   # ä¸“ä¸šç‰ˆ RPM: 30000
MAX_TPM_PRO = 5000000                 # ä¸“ä¸šç‰ˆ TPM: 5M

# æ™®é€šç‰ˆé™æµå‚æ•°
MAX_RPM = 3000                        # RPM: æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶
MAX_TPM = 5000000                     # TPM: æ¯åˆ†é’Ÿtokenæ•°é™åˆ¶ (5M)

# ä»¤ç‰Œæ¡¶é™æµ
TOKEN_BUCKET = MAX_TPM
REQUEST_BUCKET = MAX_RPM
LAST_REFILL_TIME = time.time()
TOKEN_LOCK = threading.Lock()


# ============== æ™ºèƒ½æ¨¡å‹è·¯ç”±ç¼“å­˜ ==============
class ModelRouteCache:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±ç¼“å­˜ - ç¼“å­˜æ–‡æ¡£ç‰¹å¾ä¸æ¨¡å‹é€‰æ‹©çš„æ˜ å°„"""
    
    def __init__(self, cache_file: str = None):
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()
        self._cache_file = cache_file or str(Path(__file__).parent / ".model_route_cache.json")
        self._load_cache()
    
    def _load_cache(self):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        try:
            if os.path.exists(self._cache_file):
                with open(self._cache_file, 'r', encoding='utf-8') as f:
                    self._cache = json.load(f)
                logger.info(f"å·²åŠ è½½æ¨¡å‹è·¯ç”±ç¼“å­˜: {len(self._cache)} æ¡è®°å½•")
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            self._cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            with open(self._cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _compute_hash(self, text: str, image_count: int) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        # ä½¿ç”¨æ–‡æœ¬å‰1000å­—ç¬¦ + å›¾ç‰‡æ•°é‡ç”Ÿæˆå“ˆå¸Œï¼Œå‡å°‘è®¡ç®—é‡
        content = f"{text[:1000]}|{image_count}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get(self, text: str, image_count: int) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„è·¯ç”±å†³ç­–"""
        content_hash = self._compute_hash(text, image_count)
        with self._lock:
            return self._cache.get(content_hash)
    
    def set(self, text: str, image_count: int, route_info: Dict[str, Any]):
        """ç¼“å­˜è·¯ç”±å†³ç­–"""
        content_hash = self._compute_hash(text, image_count)
        with self._lock:
            self._cache[content_hash] = {
                **route_info,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            self._save_cache()
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self._lock:
            vl_count = sum(1 for v in self._cache.values() if v.get("model") == MODEL_VL)
            long_count = sum(1 for v in self._cache.values() if v.get("model") == MODEL_LONG)
            return {
                "total": len(self._cache),
                "vl_model": vl_count,
                "long_model": long_count
            }


class ModelRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨ - æ ¹æ®æ–‡æ¡£ç‰¹å¾é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
    
    # å›¾è¡¨ç›¸å…³çš„å…³é”®è¯æ¨¡å¼
    FIGURE_PATTERNS = [
        r'Fig\.?\s*\d+',
        r'Figure\s*\d+',
        r'å›¾\s*\d+',
        r'Table\s*\d+',
        r'è¡¨\s*\d+',
        r'Chart\s*\d+',
        r'å›¾è¡¨\s*\d+',
        r'Abb\.?\s*\d+',
        r'Abbildung\s*\d+',  # å¾·è¯­
    ]
    
    # å¯èƒ½æ˜¯å›¾è¡¨çš„å¸¸è§æè¿°
    CHART_INDICATORS = [
        r'æ•°æ®æ¥æº',
        r'source.*data',
        r'æŸ±çŠ¶å›¾',
        r'é¥¼å›¾',
        r'æŠ˜çº¿å›¾',
        r'scatter\s*plot',
        r'bar\s*chart',
        r'pie\s*chart',
        r'line\s*chart',
        r'histogram',
        r'çƒ­åŠ›å›¾',
        r'heatmap',
    ]
    
    def __init__(self):
        self._cache = ModelRouteCache()
        self._stats = {"vl_calls": 0, "long_calls": 0, "cache_hits": 0}
        self._stats_lock = threading.Lock()
    
    def has_figures(self, text: str, image_paths: list) -> Tuple[bool, Dict[str, Any]]:
        """
        åˆ¤æ–­æ–‡æ¡£æ˜¯å¦åŒ…å«å›¾è¡¨
        
        Returns:
            (has_figures, analysis_info) - æ˜¯å¦æœ‰å›¾è¡¨ï¼Œä»¥åŠåˆ†æè¯¦æƒ…
        """
        analysis = {
            "text_length": len(text),
            "image_count": len(image_paths),
            "figure_mentions": [],
            "chart_indicators": [],
            "confidence": 0.0
        }
        
        # 1. ç›´æ¥æ£€æŸ¥å›¾ç‰‡æ•°é‡
        if len(image_paths) > 0:
            analysis["confidence"] = 0.9
            analysis["reason"] = "å­˜åœ¨å›¾ç‰‡æ–‡ä»¶"
            return True, analysis
        
        # 2. æ£€æŸ¥æ–‡æœ¬ä¸­çš„å›¾è¡¨å¼•ç”¨
        figure_mentions = []
        for pattern in self.FIGURE_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                figure_mentions.extend(matches[:3])  # æ¯ç§æ¨¡å¼æœ€å¤šè®°å½•3ä¸ª
        
        analysis["figure_mentions"] = list(set(figure_mentions))
        
        # 3. æ£€æŸ¥å›¾è¡¨æŒ‡ç¤ºè¯
        chart_indicators = []
        for pattern in self.CHART_INDICATORS:
            if re.search(pattern, text, re.IGNORECASE):
                chart_indicators.append(pattern)
        
        analysis["chart_indicators"] = chart_indicators
        
        # 4. è®¡ç®—ç½®ä¿¡åº¦
        if figure_mentions:
            # æœ‰å›¾è¡¨å¼•ç”¨ï¼Œå¢åŠ ç½®ä¿¡åº¦
            analysis["confidence"] = min(0.5 + len(figure_mentions) * 0.1, 0.85)
            analysis["reason"] = f"æ–‡æœ¬ä¸­å­˜åœ¨å›¾è¡¨å¼•ç”¨: {figure_mentions[:5]}"
            return True, analysis
        
        if chart_indicators:
            # æœ‰å›¾è¡¨æŒ‡ç¤ºè¯
            analysis["confidence"] = min(0.3 + len(chart_indicators) * 0.15, 0.75)
            analysis["reason"] = f"å­˜åœ¨å›¾è¡¨ç›¸å…³æè¿°: {chart_indicators}"
            return True, analysis
        
        # 5. æ— å›¾è¡¨ç‰¹å¾
        analysis["confidence"] = 0.9
        analysis["reason"] = "æœªæ£€æµ‹åˆ°å›¾è¡¨ç‰¹å¾"
        return False, analysis
    
    def route(self, text: str, image_paths: list) -> Tuple[str, Dict[str, Any]]:
        """
        æ™ºèƒ½è·¯ç”±é€‰æ‹©æ¨¡å‹
        
        Returns:
            (model_name, route_info) - é€‰æ‹©çš„æ¨¡å‹åå’Œè·¯ç”±ä¿¡æ¯
        """
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        cached = self._cache.get(text, len(image_paths))
        if cached:
            with self._stats_lock:
                self._stats["cache_hits"] += 1
            logger.info(f"ğŸ¯ è·¯ç”±ç¼“å­˜å‘½ä¸­: {cached['model']}")
            return cached["model"], cached
        
        # 2. åˆ†ææ–‡æ¡£ç‰¹å¾
        has_fig, analysis = self.has_figures(text, image_paths)
        
        # 3. é€‰æ‹©æ¨¡å‹
        if has_fig:
            model = MODEL_VL
            route_reason = f"æ£€æµ‹åˆ°å›¾è¡¨ç‰¹å¾ â†’ ä½¿ç”¨è§†è§‰æ¨¡å‹ {MODEL_VL}"
        else:
            model = MODEL_LONG
            route_reason = f"çº¯æ–‡æœ¬æ–‡æ¡£ â†’ ä½¿ç”¨é•¿æ–‡æœ¬æ¨¡å‹ {MODEL_LONG}ï¼ˆæ›´å¿«æ›´ç»æµï¼‰"
        
        # 4. æ„å»ºè·¯ç”±ä¿¡æ¯
        route_info = {
            "model": model,
            "has_figures": has_fig,
            "reason": route_reason,
            **analysis
        }
        
        # 5. æ›´æ–°ç»Ÿè®¡
        with self._stats_lock:
            if model == MODEL_VL:
                self._stats["vl_calls"] += 1
            else:
                self._stats["long_calls"] += 1
        
        # 6. ç¼“å­˜è·¯ç”±å†³ç­–
        self._cache.set(text, len(image_paths), route_info)
        
        logger.info(f"ğŸ”€ æ™ºèƒ½è·¯ç”±: {route_reason}")
        return model, route_info
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡"""
        with self._stats_lock:
            return {
                **self._stats,
                "cache_stats": self._cache.get_stats()
            }


# å…¨å±€è·¯ç”±å™¨å®ä¾‹
_model_router = ModelRouter()

def refill_token_bucket():
    """è¡¥å……ä»¤ç‰Œæ¡¶"""
    global TOKEN_BUCKET, LAST_REFILL_TIME
    current_time = time.time()
    elapsed = current_time - LAST_REFILL_TIME
    if elapsed > 60:
        TOKEN_BUCKET = MAX_TPM
        LAST_REFILL_TIME = current_time
    else:
        refill_amount = (elapsed / 60) * MAX_TPM
        TOKEN_BUCKET = min(MAX_TPM, TOKEN_BUCKET + refill_amount)
        LAST_REFILL_TIME = current_time

def wait_for_tokens(required_tokens):
    """ç­‰å¾…ç›´åˆ°ä»¤ç‰Œæ¡¶ä¸­æœ‰è¶³å¤Ÿä»¤ç‰Œ"""
    global TOKEN_BUCKET
    with TOKEN_LOCK:
        refill_token_bucket()
        while TOKEN_BUCKET < required_tokens:
            deficit = required_tokens - TOKEN_BUCKET
            wait_seconds = (deficit / MAX_TPM) * 60 + 0.1
            logger.info(f"TPMä¸è¶³ï¼Œéœ€è¦ç­‰å¾…{wait_seconds:.2f}ç§’ (éœ€æ±‚:{required_tokens} å¯ç”¨:{TOKEN_BUCKET:.0f})")
            time.sleep(wait_seconds)
            refill_token_bucket()
        
        TOKEN_BUCKET -= required_tokens
        logger.info(f"æ‰£é™¤{required_tokens} tokensï¼Œå‰©ä½™:{TOKEN_BUCKET:.0f}")

def preprocess_context(context, model: str = MODEL_VL):
    """
    å¢å¼ºå‹æ–‡æœ¬é¢„å¤„ç† - ç§»é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
    
    Args:
        context: åŸå§‹æ–‡æœ¬
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œå†³å®šä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
            - MODEL_VL: é™åˆ¶ 254K tokens
            - MODEL_LONG: é™åˆ¶ 1M tokensï¼ˆå‡ ä¹ä¸æˆªæ–­ï¼‰
            - MODEL_PRO: é™åˆ¶ 991K tokensï¼ˆä¸“ä¸šç‰ˆï¼‰
    """
    sections_to_remove = [
        r'references?',
        r'acknowledg?e?ments?',
        r'data availability',
        r'declaration of competing interest',
        r'conflict of interest',
        r'funding',
        r'appendix',
    ]
    pattern = r'(?i)\n#*\s*(' + '|'.join(sections_to_remove) + r')[\s\S]*?(\n#|$)'
    context = re.sub(pattern, '', context)
    context = re.sub(r'(?i)(\n|^)\s*acknowledg?e?ments?[\s\S]*?(\n#|$)', '', context)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šæˆªæ–­é•¿åº¦
    if model == MODEL_LONG:
        max_length = MAX_CONTEXT_LENGTH_LONG
    elif model == MODEL_PRO:
        max_length = MAX_CONTEXT_LENGTH_PRO
    else:
        max_length = MAX_CONTEXT_LENGTH_VL
    
    if len(context) > max_length:
        logger.info(f"æ–‡æœ¬æˆªæ–­: {len(context)} â†’ {max_length} (æ¨¡å‹: {model})")
    
    return context[:max_length]

def try_repair_json(json_str: str):
    """é’ˆå¯¹Qwen3-VLè¾“å‡ºçš„JSONä¿®å¤"""
    cleaned = json_str.strip()
    
    # å¤„ç†å¯èƒ½çš„è¾“å‡ºæ ¼å¼
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]
    if cleaned.startswith("```"):
        cleaned = cleaned[3:]
    if cleaned.endswith("```"):
        cleaned = cleaned[:-3]
    cleaned = cleaned.strip()
    
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
        
        # å°è¯•ä¿®å¤å¸¸è§çš„æ ¼å¼é—®é¢˜
        cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)  # å»é™¤å°¾éšé€—å·
        cleaned = re.sub(r'([{\[])\s*,', r'\1', cleaned)  # å»é™¤å¼€å¤´é€—å·
        
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            return None

def build_messages(text: str, img_abs_paths: list[str], prompt: str = None) -> list:
    """ä¸ºQwen3-VLæ„å»ºæ¶ˆæ¯æ ¼å¼ - ä½¿ç”¨systemè§’è‰²ä¼ é€’æç¤ºè¯"""
    # ä½¿ç”¨ä¼ å…¥çš„æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
    actual_prompt = prompt if prompt else PROMPT_TXT
    if not actual_prompt or not actual_prompt.strip():
        actual_prompt = "ä½ æ˜¯ä¸€ä¸ªèƒ½ä»å›¾æ–‡ä¿¡æ¯æå–æŒ‡æ ‡ä¸ºjsonçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œåªè¾“å‡ºæå–å‡ºçš„jsonä¿¡æ¯ã€‚"
        logger.warning("ä½¿ç”¨é»˜è®¤æç¤ºè¯ï¼Œå› ä¸ºæœªæä¾›æœ‰æ•ˆçš„æç¤ºè¯")
    
    # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ˆåŒ…å«æç¤ºè¯ï¼‰
    system_msg = {
        "role": "system",
        "content": [{"text": actual_prompt}]
    }
    
    # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«è®ºæ–‡æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
    user_content = [{"text": text}]
    
    # æ·»åŠ å›¾ç‰‡ - Qwen3-VLæ”¯æŒæ›´å¥½çš„å¤šå›¾åƒå¤„ç†
    for img_path in img_abs_paths:
        user_content.append({"image": f"file://{img_path}"})
    
    user_msg = {
        "role": "user", 
        "content": user_content
    }
    
    return [system_msg, user_msg]

def extract_once(md_file: str, prompt: str = None, model_mode: str = "normal") -> tuple:
    """ä½¿ç”¨æ™ºèƒ½æ¨¡å‹è·¯ç”±è¿›è¡Œæå–
    
    Args:
        md_file: Markdownæ–‡ä»¶è·¯å¾„
        prompt: åŠ¨æ€æç¤ºè¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤PROMPT_TXTï¼‰
        model_mode: æ¨¡å‹æ¨¡å¼
            - "normal": æ™®é€šç‰ˆ - æ™ºèƒ½è·¯ç”±ï¼ˆqwen3-vl-plus / qwen-longï¼‰
            - "pro": ä¸“ä¸šç‰ˆ - ç»Ÿä¸€ä½¿ç”¨ qwen3.5-plusï¼ˆæ›´å¼ºå¤§ï¼Œ991Kä¸Šä¸‹æ–‡ï¼‰
    
    Returns:
        (status, result) å…ƒç»„
    """
    try:
        # 1. è¯»å–åŸå§‹æ–‡æœ¬ï¼ˆå…ˆä¸é¢„å¤„ç†ï¼Œç”¨äºè·¯ç”±åˆ¤æ–­ï¼‰
        raw_text = open(md_file, encoding="utf-8").read()
        
        # 2. è§£æmdä¸­æ‰€æœ‰å›¾ç‰‡è·¯å¾„ï¼ˆä¸é™æ•°é‡ï¼‰
        md_dir = Path(md_file).parent
        fig_imgs = []
        
        # æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´å¥½åœ°åŒ¹é…å›¾ç‰‡å’Œå¯¹åº”çš„Figæè¿°
        pattern = re.compile(
            r'!\[\]\(images/([^)]+)\)[\s\S]*?(Fig\.|Figure|å›¾)\s?\d+[\.\d]*\..*?(\n|$)',
            re.IGNORECASE | re.MULTILINE
        )
        
        for m in pattern.finditer(raw_text):
            img_name = m.group(1)
            rel_path = f"images/{img_name}"
            abs_path = (md_dir / rel_path).resolve()
            if abs_path.exists():
                fig_imgs.append(str(abs_path))
                logger.info(f"æ‰¾åˆ°å¸¦Fig.æè¿°çš„å›¾ç‰‡: {img_name}")
            else:
                logger.warning(f"å›¾ç‰‡ä¸å­˜åœ¨: {abs_path}")

        if not fig_imgs:
            logger.info("æœªæ‰¾åˆ°å¸¦Figæè¿°çš„å›¾ç‰‡ï¼Œå°è¯•æ–‡ä»¶ååŒ…å«'fig'çš„å›¾ç‰‡")
            img_dir = md_dir / "images"
            if img_dir.exists():
                for p in img_dir.glob("*"):
                    if p.suffix.lower() in {".jpg", ".jpeg", ".png", ".bmp", ".gif"} and "fig" in p.stem.lower():
                        fig_imgs.append(str(p.resolve()))
                        logger.info(f"æ·»åŠ æ–‡ä»¶åå«'fig'çš„å›¾ç‰‡: {p.name}")
        
        # ä¸å†é™åˆ¶å›¾ç‰‡æ•°é‡ï¼Œå…¨éƒ¨ä¼ å…¥
        abs_imgs = fig_imgs
        logger.info(f"ğŸ–¼ï¸ å…±æ‰¾åˆ° {len(abs_imgs)} å¼ å›¾ç‰‡")

        # 3. ğŸš€ æ¨¡å‹é€‰æ‹©é€»è¾‘
        if model_mode == "pro":
            # ä¸“ä¸šç‰ˆï¼šç»Ÿä¸€ä½¿ç”¨ qwen3.5-plus
            selected_model = MODEL_PRO
            route_info = {
                "model": selected_model,
                "has_figures": len(abs_imgs) > 0,
                "reason": "ä¸“ä¸šç‰ˆæ¨¡å¼ â†’ ä½¿ç”¨ qwen3.5-plusï¼ˆæ›´å¼ºæ¨ç†èƒ½åŠ›ï¼Œ991Kä¸Šä¸‹æ–‡ï¼‰"
            }
            logger.info(f"ğŸ“Š ä¸“ä¸šç‰ˆæ¨¡å¼: ä½¿ç”¨ {selected_model}")
        else:
            # æ™®é€šç‰ˆï¼šæ™ºèƒ½è·¯ç”±
            selected_model, route_info = _model_router.route(raw_text, abs_imgs)
            logger.info(f"ğŸ“Š æ™ºèƒ½è·¯ç”±å†³ç­–: æ¨¡å‹={selected_model}, åŸå› ={route_info.get('reason', 'N/A')}")
        
        # 4. æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
        text = preprocess_context(raw_text, model=selected_model)
        logger.info(f"ğŸ“„ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦ (æ¨¡å‹: {selected_model})")
        
        # 5. ä¼°ç®—token
        if selected_model == MODEL_LONG:
            # qwen-long æ˜¯çº¯æ–‡æœ¬æ¨¡å‹ï¼Œä¸è®¡å…¥å›¾ç‰‡token
            estimated_tokens = len(text) // 3.5
            logger.info(f"ğŸ’° qwen-long çº¯æ–‡æœ¬ä¼°ç®—: {estimated_tokens} tokens")
        elif selected_model == MODEL_PRO:
            # qwen3.5-plus ä¸“ä¸šç‰ˆ
            estimated_tokens = len(text) // 3.5 + len(abs_imgs) * 1000
            logger.info(f"ğŸ’° qwen3.5-plus ä¼°ç®—: æ–‡æœ¬ {len(text)//3.5:.0f} + å›¾ç‰‡ {len(abs_imgs)*1000} = {estimated_tokens:.0f} tokens")
        else:
            # qwen3-vl-plus
            estimated_tokens = len(text) // 3.5 + len(abs_imgs) * 1000
            logger.info(f"ğŸ’° qwen3-vl-plus ä¼°ç®—: æ–‡æœ¬ {len(text)//3.5:.0f} + å›¾ç‰‡ {len(abs_imgs)*1000} = {estimated_tokens:.0f} tokens")
        
        # 6. æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        rsp = None
        for attempt in range(max_retries):
            try:
                wait_for_tokens(estimated_tokens)
                
                # æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºä¸åŒçš„æ¶ˆæ¯æ ¼å¼
                if selected_model == MODEL_LONG:
                    # qwen-long ä½¿ç”¨ç®€å•æ–‡æœ¬æ ¼å¼
                    messages = build_messages_for_long(text, prompt=prompt)
                else:
                    # qwen-vl ä½¿ç”¨å¤šæ¨¡æ€æ ¼å¼
                    messages = build_messages(text, abs_imgs, prompt=prompt)
                
                rsp = MultiModalConversation.call(
                    model=selected_model,
                    messages=messages,
                    temperature=0,
                    response_format={"type": "json_object"}
                )
                if rsp.status_code == 200:
                    break  # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯
                else:
                    logger.warning(f"APIè¿”å›çŠ¶æ€ç å¼‚å¸¸: {rsp.status_code} (å°è¯• {attempt+1}/{max_retries})")
            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
        
        if not rsp or rsp.status_code != 200:
            error_msg = f"APIé”™è¯¯: {getattr(rsp, 'message', 'Unknown error')}" if rsp else "APIè°ƒç”¨å¤±è´¥"
            raise RuntimeError(error_msg)
        
        # 7. è§£æè¿”å›
        content = rsp.output.choices[0].message.content
        if isinstance(content, list) and content and "text" in content[0]:
            json_str = content[0]["text"]
            
            # å…ˆå°è¯•ç›´æ¥è§£æ
            try:
                result = json.loads(json_str)
                # æ·»åŠ æ¨¡å‹è·¯ç”±ä¿¡æ¯åˆ°ç»“æœ
                result["_model_route"] = {
                    "model": selected_model,
                    "has_figures": route_info.get("has_figures"),
                    "reason": route_info.get("reason"),
                    "text_length": len(text)
                }
                return ("success", result)
            except json.JSONDecodeError:
                pass
            
            # å°è¯•ä¿®å¤
            repaired_obj = try_repair_json(json_str)
            if repaired_obj is not None:
                repaired_obj["_model_route"] = {
                    "model": selected_model,
                    "has_figures": route_info.get("has_figures"),
                    "reason": route_info.get("reason"),
                    "text_length": len(text)
                }
                return ("success", repaired_obj)
            
            # ä¿®å¤å¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
            return ("partial_data", json_str)
        else:
            return ("error", "APIè¿”å›æ ¼å¼é”™è¯¯")
            
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        return ("error", str(e))


def build_messages_for_long(text: str, prompt: str = None) -> list:
    """ä¸º qwen-long æ„å»ºæ¶ˆæ¯æ ¼å¼ï¼ˆçº¯æ–‡æœ¬ï¼Œæ— å›¾ç‰‡ï¼‰"""
    actual_prompt = prompt if prompt else PROMPT_TXT
    if not actual_prompt or not actual_prompt.strip():
        actual_prompt = "ä½ æ˜¯ä¸€ä¸ªèƒ½ä»æ–‡æœ¬ä¿¡æ¯æå–æŒ‡æ ‡ä¸ºjsonçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œåªè¾“å‡ºæå–å‡ºçš„jsonä¿¡æ¯ã€‚"
        logger.warning("ä½¿ç”¨é»˜è®¤æç¤ºè¯ï¼Œå› ä¸ºæœªæä¾›æœ‰æ•ˆçš„æç¤ºè¯")
    return [
        {
            "role": "system",
            "content": actual_prompt
        },
        {
            "role": "user", 
            "content": text
        }
    ]

def should_skip_processing(output_path, error_output_path):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å¤„ç†ï¼ˆåŒæ—¶æ£€æŸ¥æ­£å¸¸è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºï¼‰"""
    for path in [output_path, error_output_path]:
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content == '[]' or not content:
                        continue
                    if 'compressive' in content or 'error' in content:
                        return True
            except Exception:
                pass
    return False

def process_task(file_path, output_folder, error_output_folder, output_file):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ä»»åŠ¡"""
    logger.info(f"å¼€å§‹å¤„ç†: {file_path}")
    start_time = time.time()
    
    try:
        # è°ƒç”¨extract_onceå‡½æ•°
        status, result = extract_once(file_path)
        
        if status == "success":
            # ä¿å­˜æ­£å¸¸ç»“æœ
            output_file_path = os.path.join(output_folder, output_file)
            with open(output_file_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            elapsed = time.time() - start_time
            logger.info(f"å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
            return ("success", 1)
        elif status == "partial_data":
            # ä¿å­˜éƒ¨åˆ†æ•°æ®ç»“æœ
            error_data = {
                "error": "éƒ¨åˆ†æ•°æ®ä¿®å¤å¤±è´¥",
                "raw_response": result,
                "source_file": file_path,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            error_file_path = os.path.join(error_output_folder, output_file)
            with open(error_file_path, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, ensure_ascii=False, indent=2)
            logger.warning(f"éƒ¨åˆ†æ•°æ®ä¿å­˜åˆ°: {error_file_path}")
            return ("partial_error", 0)
        else:
            # ä¿å­˜é”™è¯¯ç»“æœ
            error_file_path = os.path.join(error_output_folder, output_file)
            error_data = {
                "error": status,
                "raw_response": result,
                "source_file": file_path,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(error_file_path, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, ensure_ascii=False, indent=2)
            logger.error(f"å¤„ç†å¤±è´¥ï¼Œé”™è¯¯ç»“æœå·²ä¿å­˜åˆ°: {error_file_path}")
            return ("error", 0)
    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {file_path} - {e}")
        return ("error", 0)

def main():
    md_folder = "input"
    output_folder = "output"
    error_output_folder = "output_error"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(error_output_folder, exist_ok=True)

    success_count = 0
    error_count = 0
    skipped_count = 0
    tasks = []

    # æ”¶é›†ä»»åŠ¡
    for root, dirs, files in os.walk(md_folder):
        for file in files:
            if file == "full.md":
                file_path = os.path.join(root, file)
                folder_name = os.path.basename(root)

                # æå–PDFç¼–å·æˆ–åç§°
                match = re.match(r'^(\d+)', folder_name)
                if match:
                    pdf_number = match.group(1)
                    output_file = f"{pdf_number}.txt"
                else:
                    pdf_name = folder_name.split('-')[0] + ".pdf"
                    output_file = os.path.splitext(pdf_name)[0] + ".txt"

                output_path = os.path.join(output_folder, output_file)
                error_output_path = os.path.join(error_output_folder, output_file)
                
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡å¤„ç†
                if should_skip_processing(output_path, error_output_path):
                    logger.info(f'è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {output_file}')
                    skipped_count += 1
                    continue

                tasks.append((file_path, output_folder, error_output_folder, output_file))

    logger.info(f"å…±å‘ç° {len(tasks)} ä¸ªéœ€è¦å¤„ç†çš„ä»»åŠ¡")

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    max_workers = min(3, len(tasks))  # é™åˆ¶å¹¶å‘æ•°é¿å…è¿‡è½½
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for task in tasks:
            future = executor.submit(process_task, *task)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            try:
                status, count = future.result()
                if status == "success":
                    success_count += count
                elif status == "partial_error":
                    # éƒ¨åˆ†é”™è¯¯ä¹Ÿè®¡å…¥é”™è¯¯è®¡æ•°
                    error_count += 1
                else:
                    error_count += 1
            except Exception as e:
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                error_count += 1

    logger.info(f"Qwenæ™ºèƒ½è·¯ç”±å¤„ç†æ‘˜è¦: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {error_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª")
    
    # è¾“å‡ºè·¯ç”±ç»Ÿè®¡
    stats = _model_router.get_stats()
    logger.info(f"ğŸ“Š æ¨¡å‹è·¯ç”±ç»Ÿè®¡: VLæ¨¡å‹è°ƒç”¨={stats['vl_calls']}, Longæ¨¡å‹è°ƒç”¨={stats['long_calls']}, ç¼“å­˜å‘½ä¸­={stats['cache_hits']}")
    logger.info(f"ğŸ“¦ ç¼“å­˜ç»Ÿè®¡: {stats['cache_stats']}")

if __name__ == "__main__":
    main()